# Normalization Techniques 

## Batch & Channel & Layer Normalization
<P align="center">
<img src="https://github.com/user-attachments/assets/cd06d0d0-44ab-4d6c-b1e3-9fdf046b4e83" width="60%" height="60%">
</P>

$$y = \frac{(x - mean)}{\sqrt{(var + eps)}}$$

## RMS Normalization
* Root Mean Square Layer Normalization

$$y=\frac{x}{\sqrt{(mean(x¬≤)+Œµ)}}$$


|Íµ¨Î∂Ñ|Layer Normalization|RMSNorm|
|:---:|:---:|:---:|
|Ï†ïÍ∑úÌôî Î∞©Ïãù|	ÌèâÍ∑†Í≥º Î∂ÑÏÇ∞ÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ Ï†ïÍ∑úÌôî|	Ï†úÍ≥±ÌèâÍ∑†ÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ Ï†ïÍ∑úÌôî|
|Í≥ÑÏÇ∞ ÎπÑÏö©|	ÌèâÍ∑†Í≥º Î∂ÑÏÇ∞ Í≥ÑÏÇ∞,ÎπÑÏö©Ïù¥ Îçî ÎÜíÏùå|	Ï†úÍ≥±ÌèâÍ∑† Í≥ÑÏÇ∞Îßå, ÎπÑÏö©Ïù¥ Îçî ÎÇÆÏùå|
|ÌååÎùºÎØ∏ÌÑ∞|	ùõæ (Ïä§ÏºÄÏùº ÌååÎùºÎØ∏ÌÑ∞), ùõΩ (ÏãúÌîÑÌä∏ ÌååÎùºÎØ∏ÌÑ∞)|	ùõæ (Ïä§ÏºÄÏùº ÌååÎùºÎØ∏ÌÑ∞)|
|Î∞∞Ïπò| ÎØºÍ∞êÎèÑ	ÎØºÍ∞êÌïòÏßÄ ÏïäÏùå|	ÎØºÍ∞êÌïòÏßÄ ÏïäÏùå|
|Ï£º ÏÇ¨Ïö© ÏÇ¨Î°Ä|	RNNÍ≥º Í∞ôÏùÄ ÏàúÌôò Ïã†Í≤ΩÎßù	|ÎåÄÍ∑úÎ™® Ïã†Í≤ΩÎßù, Í≥ÑÏÇ∞ ÎπÑÏö©Ïù¥ ÎÇÆÏùÄ Î™®Îç∏ (transformer)|
|Ïû•Ï†ê|	Í∞Å ÏÉòÌîåÏùò ÌèâÍ∑†Í≥º Î∂ÑÏÇ∞ÏùÑ Í≥†Î†§, Ï†ïÍ∑úÌôî, ÏïàÏ†ïÏ†ÅÏù∏ ÌïôÏäµ Í∞ÄÎä•|	Í≥ÑÏÇ∞ ÎπÑÏö©Ïù¥ ÎÇÆÍ≥† Í∞ÑÎã®Ìïú Í≥ÑÏÇ∞ Ï†ïÍ∑úÌôî|
|Îã®Ï†ê|	ÌèâÍ∑†Í≥º Î∂ÑÏÇ∞ Í≥ÑÏÇ∞ÏúºÎ°ú Ïù∏Ìï¥ Í≥ÑÏÇ∞ ÎπÑÏö©Ïù¥ ÎÜíÏùå|	ÌèâÍ∑†ÏùÑ Í≥†Î†§ÌïòÏßÄ ÏïäÍ∏∞ ÎïåÎ¨∏Ïóê ÏùºÎ∂Ä Í≤ΩÏö∞ÏóêÏÑú ÌïôÏäµ ÏïàÏ†ïÏÑ± Îñ®Ïñ¥Ïßà Ïàò ÏûàÏùå|

# Standardization Techniques


## Data standardization


## Z-score standardization


# Adaptive Layer Normalization (AdaLN)
## DiT Block with AdaLN-Zero 
$$ y = (\gamma * x) >> \beta $$

## AdaNorm $\Phi(y)$
1. $\Phi(y)$ ÎØ∏Î∂Ñ Í∞ÄÎä•
2. ÌèâÍ∑† scaling wiehgt Í≥†Ï†ï(fixed)
3. z ÌèâÍ∑† bounded(ÌäπÏ†ï Î≤îÏúÑ)

$$ \Phi(y_i) = C(1-\frac{y_i}{10}) $$

$$ z=C(1-ky)\odot y, y=\frac{x-\mu}{\sigma}, \mu = \frac{1}{H} \displaystyle\sum^H_{i=1}x_i, \sigma = \sqrt{\frac{1}{H}\displaystyle\sum_{i=1}^{H}(x_i - \mu)^2}$$ 

* $C = hyper parameter, k = 1/10$


# Adaptive Instance Normalization
Content Input $x$, Style Input $y$
* Simply align the channel-wise mean and variance of x to match those of y
* AdaIN has no learnable affine parameters.

$$ AdaIN(x,y) = \sigma(y)\left(\frac{x-\mu(x)}{\sigma(x)}\right) + \mu(y) $$


# Normalization

* Training Parameter

$$ \hat{x_i} = \frac{x_i - \mu}{\sqrt{\sigma^2 + \epsilon}} $$ 
$$ y_i = \gamma\hat{x} + \beta $$
$$ \gamma = scale, \beta = shift, Both \ training \ parameter $$

* Variance

$$ \sigma^2 = \frac{1}{m}\displaystyle\sum_{i=1}^{m}(x_i-\mu)^2 $$  
$$ \sigma^2 = \frac{\sum(x^2)}{N} - \left(\frac{\sum(x)}{N}\right)^2 $$
