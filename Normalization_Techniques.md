# Normalization Techniques 

## Batch & Channel & Layer Normalization
<P align="center">
<img src="https://github.com/user-attachments/assets/cd06d0d0-44ab-4d6c-b1e3-9fdf046b4e83" width="60%" height="60%">
</P>

$$y = \frac{(x - mean)}{\sqrt{(var + eps)}}$$

## RMS Normalization
* Root Mean Square Layer Normalization

$$y=\frac{x}{\sqrt{(mean(xÂ²)+Îµ)}}$$


|êµ¬ë¶„|Layer Normalization|RMSNorm|
|:---:|:---:|:---:|
|ì •ê·œí™” ë°©ì‹|	í‰ê· ê³¼ ë¶„ì‚°ì„ ì‚¬ìš©í•˜ì—¬ ì •ê·œí™”|	ì œê³±í‰ê· ì„ ì‚¬ìš©í•˜ì—¬ ì •ê·œí™”|
|ê³„ì‚° ë¹„ìš©|	í‰ê· ê³¼ ë¶„ì‚° ê³„ì‚°,ë¹„ìš©ì´ ë” ë†’ìŒ|	ì œê³±í‰ê·  ê³„ì‚°ë§Œ, ë¹„ìš©ì´ ë” ë‚®ìŒ|
|íŒŒë¼ë¯¸í„°|	ğ›¾ (ìŠ¤ì¼€ì¼ íŒŒë¼ë¯¸í„°), ğ›½ (ì‹œí”„íŠ¸ íŒŒë¼ë¯¸í„°)|	ğ›¾ (ìŠ¤ì¼€ì¼ íŒŒë¼ë¯¸í„°)|
|ë°°ì¹˜| ë¯¼ê°ë„	ë¯¼ê°í•˜ì§€ ì•ŠìŒ|	ë¯¼ê°í•˜ì§€ ì•ŠìŒ|
|ì£¼ ì‚¬ìš© ì‚¬ë¡€|	RNNê³¼ ê°™ì€ ìˆœí™˜ ì‹ ê²½ë§	|ëŒ€ê·œëª¨ ì‹ ê²½ë§, ê³„ì‚° ë¹„ìš©ì´ ë‚®ì€ ëª¨ë¸ (transformer)|
|ì¥ì |	ê° ìƒ˜í”Œì˜ í‰ê· ê³¼ ë¶„ì‚°ì„ ê³ ë ¤, ì •ê·œí™”, ì•ˆì •ì ì¸ í•™ìŠµ ê°€ëŠ¥|	ê³„ì‚° ë¹„ìš©ì´ ë‚®ê³  ê°„ë‹¨í•œ ê³„ì‚° ì •ê·œí™”|
|ë‹¨ì |	í‰ê· ê³¼ ë¶„ì‚° ê³„ì‚°ìœ¼ë¡œ ì¸í•´ ê³„ì‚° ë¹„ìš©ì´ ë†’ìŒ|	í‰ê· ì„ ê³ ë ¤í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì— ì¼ë¶€ ê²½ìš°ì—ì„œ í•™ìŠµ ì•ˆì •ì„± ë–¨ì–´ì§ˆ ìˆ˜ ìˆìŒ|

# Standardization Techniques


## Data standardization


## Z-score standardization


# Adaptive Layer Normalization (AdaLN)
## DiT Block with AdaLN-Zero 
$$ y = (\gamma * x) >> \beta $$

## AdaNorm $\Phi(y)$
1. $\Phi(y)$ ë¯¸ë¶„ ê°€ëŠ¥
2. í‰ê·  scaling wiehgt ê³ ì •(fixed)
3. z í‰ê·  bounded(íŠ¹ì • ë²”ìœ„)

$$ \Phi(y_i) = C(1-\frac{y_i}{10}) $$

$$ z=C(1-ky)\odot y, y=\frac{x-\mu}{\sigma}, \mu = \frac{1}{H} \displaystyle\sum^H_{i=1}x_i, \sigma = \sqrt{\frac{1}{H}\displaystyle\sum_{i=1}^{H}(x_i - \mu)^2}$$ 

* $C = hyper parameter, k = 1/10$


# Adaptive Instance Normalization
Content Input $x$, Style Input $y$

$$ AdaIN(x,y) = \sigma(y)\left(\frac{x-\mu(x)}{\sigma(x)}\right) + \mu(y) $$

